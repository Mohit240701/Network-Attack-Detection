# -*- coding: utf-8 -*-
"""Grey Box Attack on CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D92nkVVePpDu2PbxPpunyUqBgYyLMtbg
"""

import os
import re
import time
import pandas as pd
import numpy as np
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
import seaborn as sns
import random

import matplotlib.pyplot as plt

df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Dataset_S022Final.csv")
# Dropped sendTime, sender, reciever, IP_src, port_src, IP_dest, port_dest, Frequency
df=df[['Next_Current_diff','Next_Pre_diff','SNext_Current_diff','SNext_Pre_diff','rcvdPK','duration(ms)','packet_type','droppedPKWrongPort','sentPK','size','channel','DataQueueLen','passedUpPk','rcvdPKFromHL','rcvdPKFromLL','sentDownPK','DropPKByQueue','snir','throughput','label']]
# One hot encoding label
df = pd.get_dummies(df, columns = ['label'])
df.describe()

X = df.drop(columns = ['label_BROUILLAGE_Trafic','label_DDOS_UDP_FLOOD','label_Normal','label_PLUIES_ET_ORAGES'])
y = df[['label_BROUILLAGE_Trafic','label_DDOS_UDP_FLOOD','label_Normal','label_PLUIES_ET_ORAGES']]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)
X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=1)

import tensorflow as tf

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Conv1D(filters = 32, kernel_size = 3, activation='relu', input_shape=(19, 1)))
model.add(tf.keras.layers.MaxPooling1D(2))
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(64, activation='relu'))
model.add(tf.keras.layers.Dense(4))

model.summary()

model.compile(optimizer='adam',
              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history = model.fit(X_train,
                    y_train,
                    epochs=5,
                    validation_data=(X_valid, y_valid)
                    )

from sklearn.metrics import f1_score
y_predict = model.predict(X_test)
for j in range(len(y_predict)):
    curr_row = y_predict[j][:]
    max_val = max(curr_row)
    max_loc = np.where(curr_row == max_val)[0]
    for k in range(len(curr_row)):
      y_predict[j][k] = 0
    y_predict[j][max_loc] = 1
f1_baseline = f1_score(y_test, y_predict, average = 'weighted')

print(f1_baseline)

X_train_drop = X_train.drop(X_train.columns[0], axis = 1)
X_train_drop.describe()

features = ['Next_Current_diff','Next_Pre_diff','SNext_Current_diff','SNext_Pre_diff','rcvdPK','duration(ms)','packet_type','droppedPKWrongPort','sentPK','size','channel','DataQueueLen','passedUpPk','rcvdPKFromHL','rcvdPKFromLL','sentDownPK','DropPKByQueue','snir','throughput']
feature_importance = pd.DataFrame()

model_drop = tf.keras.models.Sequential()
model_drop.add(tf.keras.layers.Conv1D(filters = 32, kernel_size = 3, activation='relu', input_shape=(18, 1)))
model_drop.add(tf.keras.layers.MaxPooling1D(2))
model_drop.add(tf.keras.layers.Flatten())
model_drop.add(tf.keras.layers.Dense(64, activation='relu'))
model_drop.add(tf.keras.layers.Dense(4))

model_drop.compile(optimizer='adam',
              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

for i in range(19):
  X_train_drop = X_train.drop(X_train.columns[i], axis = 1)
  X_test_drop = X_test.drop(X_test.columns[i], axis = 1)
  X_valid_drop = X_valid.drop(X_valid.columns[i], axis = 1)

  model_drop.fit(X_train_drop,
                           y_train,
                           epochs=5,
                           validation_data=(X_valid_drop, y_valid))

  y_predict = model_drop.predict(X_test_drop)
  for j in range(len(y_predict)):
    curr_row = y_predict[j][:]
    max_val = max(curr_row)
    max_loc = np.where(curr_row == max_val)[0]
    for k in range(len(curr_row)):
      y_predict[j][k] = 0
    y_predict[j][max_loc] = 1
  f1_drop = f1_score(y_test, y_predict, average = 'weighted')

  feature_importance.insert(0, features[i], [f1_baseline-f1_drop])

print(feature_importance)

DDOS = df[df['label_DDOS_UDP_FLOOD']==1]
Brouillage = df[df['label_BROUILLAGE_Trafic']==1]
Normal = df[df['label_Normal']==1]
Pluies = df[df['label_PLUIES_ET_ORAGES']==1]

sns.histplot(DDOS['channel'])

sns.histplot(Normal['channel'])

sns.histplot(Brouillage['channel'])

sns.histplot(Pluies['channel'])

sns.histplot(DDOS['size'])

sns.histplot(Normal['size'])

sns.histplot(Brouillage['size'])

sns.histplot(Pluies['size'])

df_mod = df*1
for i in DDOS.index:
  df_mod.at[i, 'size'] = random.uniform(0, 0.12)

model = tf.keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/CNN_Classification.keras')

from sklearn.metrics import f1_score, classification_report
y_test = df
y_test = y_test[['label_BROUILLAGE_Trafic','label_DDOS_UDP_FLOOD','label_Normal','label_PLUIES_ET_ORAGES']]

df_mod_X = df_mod.drop(columns = ['label_BROUILLAGE_Trafic','label_DDOS_UDP_FLOOD','label_Normal','label_PLUIES_ET_ORAGES'])
y_predict = model.predict(df_mod_X)
for j in range(len(y_predict)):
    curr_row = y_predict[j][:]
    max_val = max(curr_row)
    max_loc = np.where(curr_row == max_val)[0]
    for k in range(len(curr_row)):
      y_predict[j][k] = 0
    y_predict[j][max_loc] = 1
acc = f1_score(y_test, y_predict, average = 'weighted')

print(acc)