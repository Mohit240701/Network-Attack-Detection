# -*- coding: utf-8 -*-
"""Model_Lstm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bp3yLsqJ36xndQMZ9H9JRyz2Cy6ceQhn
"""

import pandas as pd

df=pd.read_csv("/content/drive/MyDrive/Dataset_S022Final.csv")

from google.colab import drive
drive.mount('/content/drive')

df=df[['Next_Current_diff','Next_Pre_diff','SNext_Current_diff','SNext_Pre_diff','rcvdPK','duration(ms)','packet_type','droppedPKWrongPort','sentPK','size','channel','DataQueueLen','passedUpPk','rcvdPKFromHL','rcvdPKFromLL','sentDownPK','DropPKByQueue','snir','throughput','label']]

df1=df.sample(n=30000)
df1

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.src.utils.np_utils import to_categorical
# Load the dataset
data = df
# Preprocessing
data = data.dropna()  # Drop rows with missing values


# Extract features and target variable
X = data.drop('label', axis=1)
y = data['label']

X = pd.get_dummies(X)  # One-hot encode categorical variables



label_encoder = LabelEncoder()
data['label'] = label_encoder.fit_transform(data['label'])

# Scale the data
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Encode the target variable
encoder = LabelEncoder()
encoder.fit(y)
encoded_y = encoder.transform(y)
dummy_y = to_categorical(encoded_y)

# Train-test split
#X_train, X_test, y_train, y_test = train_test_split(X, dummy_y, test_size=0.2, random_state=42)


X_train, X_temp, y_train, y_temp = train_test_split(X, dummy_y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Reshape input data to 3D for LSTM
X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])
X_val = X_val.reshape(X_val.shape[0], 1, X_val.shape[1])
X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])

# Reshape input data to 3D for LSTM
#X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])
#X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])

# Build the LSTM model
model = Sequential()
model.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2])))
model.add(Dropout(0.2))
model.add(Dense(50, activation='relu'))
model.add(Dense(4, activation='softmax'))  # Adjust based on the number of classes in the target


#Compile
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Fit the model with validation data
model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_val, y_val), verbose=2)

from joblib import dump, load
dump(model, 'LSTM.joblib')

y_pred = model.predict(X_test)
y_pred = np.argmax(y_pred, axis=1)
y_test = np.argmax(y_test, axis=1)

from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')

f1_og=f1_score(y_test, y_pred, average='weighted')
f1_og

model.summary()

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix


cm = confusion_matrix(y_test, y_pred)
print(cm)
# Create a heatmap
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

from sklearn.metrics import f1_score
import tensorflow as tf

def generate_adversarial_pgd(model, X, y, epsilon=0.3, alpha=0.01, num_iter=40):
    X_adv = tf.identity(X)

    for i in range(num_iter):
        with tf.GradientTape() as tape:
            tape.watch(X_adv)
            loss = tf.keras.losses.categorical_crossentropy(y, model(X_adv, training=False))

        gradient = tape.gradient(loss, X_adv)
        perturbation = tf.stop_gradient(alpha * tf.sign(gradient))
        X_adv = X_adv + perturbation
        X_adv = tf.clip_by_value(X_adv, X - epsilon, X + epsilon)

    return X_adv

# Generate stronger adversarial examples using PGD
y_test_categorical = to_categorical(y_test, num_classes=4)  # Convert labels to one-hot encoding
adversarial_samples_pgd = generate_adversarial_pgd(model, X_test, y_test_categorical, epsilon=0.5, alpha=0.02, num_iter=1000)

X_test_combined_pgd = np.concatenate([X_test, adversarial_samples_pgd], axis=0)
y_test_combined_pgd = np.concatenate([y_test_categorical, y_test_categorical[:adversarial_samples_pgd.shape[0]]], axis=0)

# Evaluate the accuracy on the combined dataset
_, accuracy_combined_pgd = model.evaluate(X_test_combined_pgd, y_test_combined_pgd, verbose=0)
y_pred_combined_pgd = np.argmax(model.predict(X_test_combined_pgd), axis=1)
y_true_combined_pgd = np.argmax(y_test_combined_pgd, axis=1)
f1_combined_pgd = f1_score(y_true_combined_pgd, y_pred_combined_pgd, average='weighted')
print(f"Accuracy on the combined dataset (original + PGD adversarial): {accuracy_combined_pgd}")
print(f"F1-Score on the combined dataset (original + PGD adversarial): {f1_combined_pgd}")

import matplotlib.pyplot as plt
from matplotlib.ticker import PercentFormatter

labels = ['Original', 'Combined (PGD)']
accuracy_values = [accuracy, accuracy_combined_pgd]
f1_values = [f1_og, f1_combined_pgd]

x = np.arange(len(labels))
width = 0.35

fig, ax = plt.subplots()
rects1 = ax.bar(x - width / 2, accuracy_values, width, label='Accuracy')
rects2 = ax.bar(x + width / 2, f1_values, width, label='F1 Score')

ax.set_xlabel('Datasets')
ax.set_ylabel('Scores')
ax.set_title('Comparison of Accuracy and F1 Score')
ax.set_xticks(x)
ax.yaxis.set_major_formatter(PercentFormatter(1))
ax.set_xticklabels(labels)
ax.legend()

fig.tight_layout()

plt.show()